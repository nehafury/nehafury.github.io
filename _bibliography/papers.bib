---
---

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and Hegde, Chinmay and LeCun, Yann and Goldstein, Tom and Neiswanger, Willie and Goldblum, Micah},
  journal={International Conference on Learning Representations (ICLR)},
  year={2025},
  selected = {true},
  html={https://arxiv.org/abs/2406.19314},
  abstract={Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.},
  preview={livebench.png}
}


@article{marek2025small,
  title={MSE-Break: Steering Internal Representations to Bypass Refusals in Large Language Models},
  author={Saraswatula, Ashwin and Balabhadra, Pranav and Dhinkar, Pranav},
  journal={ICML 2025 Actionable Interpretability},
  year={2025},
  selected = {true},
  html={https://arxiv.org/abs/2507.07101},
  abstract={ Theflexibilityofinternalconceptembeddingsinlargelanguagemodels(LLMs)
 enablesadvancedcapabilitieslikein-context learning—butalsoopensthedoor
 toadversarialexploitation.WeintroduceMSE-Break, ajailbreakmethodthat
 optimizesasoft-promptprefixviagradientdescenttominimizethemeansquared
 error(MSE)betweenharmfulconceptembeddingsinrefusedandacceptedcon
texts.Theresultingsoftpromptpisconcept-specificbutprompt-general,enabling
 ittojailbreakawiderangeofqueriesinvolvingthatconceptwithoutfurthertun
ing.Appliedtofourpopularopen-sourceLLMs—includingGemma-2B-ITand
 LLaMA-3.1-8B-IT—MSE-Breakachievesattacksuccessratesexceeding90%. Its
 interpretability-drivendesignenablesMSE-Breaktooutperformexistingmethods
 likeGCGandAutoDAN—whileconverginginafractionofthetime.Wefindthat
 harmfulconceptembeddingsarelinearlyseparablebetweenrefusedandaccepted
 contexts—structurethatMSE-Breakactivelyexploits.Wefurthershowthatcon
ceptrepresentationscanbedrasticallysteeredin-contextwithaslittleasasingle
 token.OurfindingsunderscorethebrittlenessofLLMrepresentations—andtheir
 susceptibilitytotargetedmanipulation—highlightingtheurgencyformorerobust
 andinterpretablesafetymechanisms},
  preview={small_batch.png}
}
